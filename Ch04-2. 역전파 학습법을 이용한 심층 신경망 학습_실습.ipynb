{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 쉽게 배우는 역전파 학습법\n",
    "\n",
    "## 심층 신경망의 구조\n",
    "\n",
    "### 신경망 구조\n",
    "= 네트워크 구조(Network Architecture)\n",
    "- Neural Network는 다양한 구조(Architecture)를 가질 수 있다.\n",
    "\n",
    "#### 뉴런 Neuron\n",
    "신경망의 기본 단위, 이를 조합하여 복잡한 구조를 이룬다.\n",
    "입력(input node) -> 가중치와 편향(Weights and bias) -> 활성함수(Activation function) -> 출력(Output node)\n",
    "\n",
    "- 얕은 신경망(Shallow Neural Network)\n",
    " - 가장 단순하고 얕은(은닉 계층이 1개인) 신경망 구조\n",
    "\n",
    "- 심층 신경망(Deep Neural Network ; DNN)\n",
    " - 얕은 신경망보다 은닉 계층이 많은 신경망을 DNN이라고 부른다.\n",
    " - 보통 5개 이상의 계층이 있는 경우 '깊다'(Deep 하다)라고 표현\n",
    " - 은닉 계층 추가 = 특징의 비선형 변환 추가\n",
    " - 학습 매개변수의 수가 계층 크기의 제곱에 비례\n",
    " - Sigmoid 활성 함수 동작이 원활하지 않음(기울기 소실 문제)\n",
    "  - --> ReLU(Rectified Linear Unit)도입 필요\n",
    "\n",
    "### 역전파 학습법\n",
    "\n",
    "#### 알고리즘의 학습과 미분\n",
    "- 손실 함수를 최소화 하는 매개변수 찾기\n",
    "- 경사하강법을 위해 미분 필요\n",
    " - 손실 함수를 모든 매개변수로 미분!\n",
    " - 학습 환경이 주어졌을때, 손실 함수를 매개 변수로 여러 번 미분해야 한다\n",
    "\n",
    "#### 동적 계획법 Dynamic Programming\n",
    "- 첫 계산 시 값을 저장하므로 중복 계산이 발생하지 않는다.\n",
    "\n",
    "#### 연쇄 법칙 Chain Rule\n",
    "- 연속된 두 함수의 미분은, 각 함수의 미분을 연쇄적으로 곱한것과 같다.\n",
    "\n",
    "#### 심층 신경망의 미분 \n",
    "- 출력계층의 미분 : 연쇄 법칙을 이용하려면 손실 함수의 미분이 필요하다.(loss function을 trainable variable로 전부 미분을 해야함)\n",
    "- 마지막 은닉 계층의 미분 : 연쇄 법칙을 이용하려면 손실함수, 출력 계층의 미분이 필요하다. 출력 계층, 손실 함수의 미분을 저장해 두면(동적 계획법) 중복 연산을 피할 수 있다.\n",
    "- 은닉 계층의 미분 : 연쇄 법칙을 이용하려면 손실함수, 출력 계층, 사이의 모든 은닉 계층의 미분이 필요하다.\n",
    "\n",
    "#### 순방향 추론 Forward Inference\n",
    "- 현재 매개변수에서의 손실 값을 계산하기 위해 순차적인 연산을 수행하는 것을 *순방향 추론*이라 한다.\n",
    "- 학습을 마친 후 알고리즘을 사용할 때에는 순방향 추론을 사용한다.\n",
    "\n",
    "#### 역전파 학습법 Back-Propagation\n",
    "- 심층 신경망의 미분을 계산하기 위해, 연쇄 법칙과 동적 계획법을 이용하여 효율적으로 계산할 수 있다.\n",
    "- 이 과정이 순방향 추론과 반대로 이루어지기 때문에 이를 역전파 학습법이라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수식\n",
    "#### 뉴런 Neuron\n",
    "\n",
    "여러개의 입력을 받아서 가중치를 곱해 합해주고 nonlinear activation function을 활용해 출력으로 변환해주는게 neuron의 기본 network\n",
    "\n",
    "뉴런은 두 벡터의 내적으로 쉽게 표현할 수 있음\n",
    "\n",
    "#### 전결합 계층 Fully connected layer\n",
    "\n",
    "같은 서열에 있는 뉴런들을 하나하나 계층으로 묶어주고 두 계층간의 연결을 모든 뉴런들끼리 해준 것\n",
    "FC Layer의 연산은 행렬곱 연산으로 표현할 수 있음\n",
    "\n",
    "#### 심층 신경망 Deep Neural Network\n",
    "\n",
    "FC Layer를 연쇄적으로 적용하면 심층 신경망의 수학적 표현이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 블랙박스 모델\n",
    "- 매개 변수에 의해 동작이 달라짐\n",
    "- 학습목표 : 손실을 최소화하는 매개 변수를 찾는다.\n",
    "\n",
    "#### 수치적 기울기 Numerical Gradient\n",
    "- 미분의 정의로부터 극한 연산을 근사해 수치적 기울기를 구할 수 있다.\n",
    "- 입실론의 값이 충분이 작다면 수치적 기울기를 미분 값으로 사용할 수 있다.\n",
    "\n",
    "#### 블랙박스 모델의 수치적 기울기\n",
    "- 각 스칼라 변수를 각각 조금씩 바꾸어 대입해 보면서 수치적 기울기를 구한다.\n",
    "- N개의 매개 변수로 미분하기 위해 (N+1)번 더 손실함수를 평가해야 한다.(기준점 1번+n개의 매개변수에 대한 연산)\n",
    "\n",
    "#### 심층 신경망의 수치적 기울기\n",
    "- 손실 함수 평가에 필요한 곱연산 = N(매개변수의 개수)\n",
    " - w의 개수만큼의 곱이 있는것 (+bias도 있지만 근사값으로,,)\n",
    " - 심층 신경망의 파라미터(매개변수) 개수 >> 10000개\n",
    "- 미분을 위해 필요한 손실 평가 횟수 = N+1\n",
    "\n",
    "==> 경사 하강법 한스템 계산을 위한 연산은 N(N+1)번의 곱하기 연산이다.\n",
    "- 10만개의 파라미터를 가진 경우 무려 100억회 -> 무언가 대책이 필요하다. -> 역전파 학습법\n",
    "\n",
    "#### 연쇄 법칙 Chain Rule\n",
    "연쇄 법칙을 이용하면 연속된 함수의 미분을 각각의 미분의 곱으로 표현할 수 있다.\n",
    "\n",
    "### 역전파 학습법 Back-Propagation\n",
    "\n",
    "#### 합성 함수로서의 심층 신경망\n",
    "(입력값을 주면 출력이 나오는)\n",
    "- 심층 신경망의 각 Layer을 하나의 함수로 본다면, 신경망을 합성함수로 표현할 수 있다.\n",
    "\n",
    "#### 학습 관점에서 본 심층 신경망\n",
    "(모델이 있고 학습 데이터가 있고 로스function이 있으면 trainable parameter를 입력으로 보고 출력을 손실값으로 봄)\n",
    "- weight, bias 파라메터들이 조건부 입력으로 들어감\n",
    "- 이미 손실을 구했다면, 데이터셋의 입력과 출력은 학습 과정에서 중요하지 않다.\n",
    "- 손실을 최소화하는 파라미터만 찾으면 되기 때문!\n",
    "\n",
    "#### 심층신경망의 연쇄 법칙\n",
    "- 미분하고자 하는 경로 사이에 있는 모든 미분 값을 곱하면 원하는 미분을 구할 수 있다.\n",
    "- 원하는 미분을 구하려면 경로 사이에 있는 모든 미분 값을 알아야 한다.\n",
    "\n",
    "#### 전결합 계층의 미분\n",
    "- Hn = a(Wn x Hn-1 + bn)\n",
    "\n",
    "#### Sigmoid 함수의 미분\n",
    "- sigmoid(x)(1-sigmoid(x))\n",
    " - 초창기 신경망에 가장 많이 쓰인 sigmoid 활성함수의 미분\n",
    " - 정리된 결과를 이용하면 매우 간단하게 미분할 수 있다.\n",
    " \n",
    "### 역전파 알고리즘\n",
    "- 학습 데이터로 정방향 연산을 하여 Loss를 구한다\n",
    "- 정방향 연산시, 계층별로 BP에 필요한 중간 결과를 저장한다.\n",
    "- Loss를 각 파라미터로 미분한다. 연쇄법칙(역방향 연산)을 이용한다.\n",
    " - 마지막 계층부터 하니씩 이전 계층으로 연쇄적으로 계산한다.\n",
    " - 역방향 연산 시, 정방향 연산에서 저장한 중간 결과를 사용한다.\n",
    " \n",
    "- 미분의 연쇄 법칙과 각 함수의 수식적 미분을 이용하면, 단 한번의 손실함수 평가로 미분을 구할 수 있다.\n",
    "- 단, 중간 결과를 저장해야 하므로 메모리를 추가로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수치 미분을 이용한 심층 신경망 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유틸리티 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _t(x):\n",
    "    return np.transpose(x)\n",
    "\n",
    "def _m(A, B):\n",
    "    return np.matmul(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        # TODO\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # TODO\n",
    "\n",
    "    def grad(self):\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError:\n",
    "    def __init__(self):\n",
    "        # TODO\n",
    "\n",
    "    def __call__(self, h, y):\n",
    "        # TODO\n",
    "\n",
    "    def grad(self):\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴런 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, W, b, a_obj):\n",
    "        # TODO\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # TODO\n",
    "\n",
    "    def grad(self):\n",
    "        # TODO\n",
    "\n",
    "    def grad_W(self, dh):\n",
    "        # TODO\n",
    "\n",
    "    def grad_b(self, dh):\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 심층신경망 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self, hidden_depth, num_neuron, input, output, activation=Sigmoid):\n",
    "        def init_var(i, o):\n",
    "            return np.random.normal(0.0, 0.01, (i, o)), np.zeros((o,))\n",
    "\n",
    "        self.sequence = list()\n",
    "        # First hidden layer\n",
    "        W, b = init_var(input, num_neuron)\n",
    "        self.sequence.append(Neuron(W, b, activation))\n",
    "\n",
    "        # Hidden Layers\n",
    "        for index in range(hidden_depth):\n",
    "            W, b = init_var(num_neuron, num_neuron)\n",
    "            self.sequence.append(Neuron(W, b, activation))\n",
    "\n",
    "        # Output Layer\n",
    "        W, b = init_var(num_neuron, output)\n",
    "        self.sequence.append(Neuron(W, b, activation))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.sequence:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def calc_gradient(self, loss_obj):\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경사하강 학습법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(network, x, y, loss_obj, alpha=0.01):\n",
    "    loss = loss_obj(network(x), y)  # Forward inference\n",
    "    network.calc_gradient(loss_obj)  # Back-propagation\n",
    "    for layer in network.sequence:\n",
    "        layer.W += -alpha * layer.dW\n",
    "        layer.b += -alpha * layer.db\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동작 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(0.0, 1.0, (10,))\n",
    "y = np.random.normal(0.0, 1.0, (2,))\n",
    "\n",
    "t = time.time()\n",
    "dnn = DNN(hidden_depth=5, num_neuron=32, input=10, output=2, activation=Sigmoid)\n",
    "loss_obj = MeanSquaredError()\n",
    "for epoch in range(100):\n",
    "    loss = gradient_descent(dnn, x, y, loss_obj, alpha=0.01)\n",
    "    print('Epoch {}: Test loss {}'.format(epoch, loss))\n",
    "print('{} seconds elapsed.'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
